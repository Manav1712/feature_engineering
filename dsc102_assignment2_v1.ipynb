{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 DSC 102 FA23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment we will conduct data engineering for the Amazon dataset. It is divided into 2 parts. The extracted features in Part 1 will be used for the Part 2 of assignment, where you train a model (or models) to predict user ratings for a product.\n",
    "\n",
    "We will be using Apache Spark for this assignment. The default Spark API will be DataFrame, as it is now the recommended choice over the RDD API. That being said, please feel free to switch back to the RDD API if you see it as a better fit for the task. We provide you an option to request RDD format to start with. Also you can switch between DataFrame and RDD in your solution. \n",
    "\n",
    "Another newer API is Koalas, which is also avaliable. However, it has constraints and is not applicable to most tasks. Refer to the PA statement for detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the following parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T18:47:30.022030Z",
     "start_time": "2020-02-03T18:47:30.019499Z"
    }
   },
   "outputs": [],
   "source": [
    "PID = 'A16908447' # your pid, for instance: 'a43223333'\n",
    "INPUT_FORMAT = 'dataframe' # choose a format of your input data, valid options: 'dataframe', 'rdd', 'koalas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T18:50:16.780690Z",
     "start_time": "2020-02-03T18:47:30.263681Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/06/10 00:15:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/opt/bitnami/spark/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets ...Done\n"
     ]
    }
   ],
   "source": [
    "# Boiler plates, do NOT modify\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import getpass\n",
    "from pyspark.sql import SparkSession\n",
    "from utilities import SEED\n",
    "from utilities import PA2Test\n",
    "from utilities import PA2Data\n",
    "from utilities import data_cat\n",
    "from pa2_main import PA2Executor\n",
    "import time\n",
    "if INPUT_FORMAT == 'dataframe':\n",
    "    import pyspark.ml as M\n",
    "    import pyspark.sql.functions as F\n",
    "    import pyspark.sql.types as T\n",
    "if INPUT_FORMAT == 'koalas':\n",
    "    import databricks.koalas as ks\n",
    "elif INPUT_FORMAT == 'rdd':\n",
    "    import pyspark.mllib as M\n",
    "    from pyspark.mllib.feature import Word2Vec\n",
    "    from pyspark.mllib.linalg import Vectors\n",
    "    from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--py-files utilities.py,assignment2.py \\\n",
    "--deploy-mode client \\\n",
    "pyspark-shell'\n",
    "\n",
    "class args:\n",
    "    review_filename = data_cat.review_filename\n",
    "    product_filename = data_cat.product_filename\n",
    "    product_processed_filename = data_cat.product_processed_filename\n",
    "    ml_features_train_filename = data_cat.ml_features_train_filename\n",
    "    ml_features_test_filename = data_cat.ml_features_test_filename\n",
    "    output_root = '/home/{}/{}-pa2/test_results'.format(getpass.getuser(), PID)\n",
    "    test_results_root = data_cat.test_results_root\n",
    "    pid = PID\n",
    "\n",
    "pa2 = PA2Executor(args, input_format=INPUT_FORMAT)\n",
    "data_io = pa2.data_io\n",
    "data_dict = pa2.data_dict\n",
    "begin = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-26T20:45:52.353249Z",
     "start_time": "2020-01-26T20:45:52.343036Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import your own dependencies\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Bring the part_1 datasets to memory and de-cache part_2 datasets. \n",
    "# Execute this once before you start working on this Part\n",
    "data_dict, _ = data_io.cache_switch(data_dict, 'part_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task0: warm up \n",
    "This task is provided for you to get familiar with Spark API. We will use the dataframe API to demonstrate. Solution is given to you and this task won't be graded.\n",
    "\n",
    "Refer to https://spark.apache.org/docs/latest/api/python/pyspark.sql.html for API guide.\n",
    "\n",
    "The task is to implement the function below. Given the ```product_data``` table:\n",
    "1. Take and print five rows.\n",
    "\n",
    "1. Select only the ```asin``` column, then print five rows of it.\n",
    "\n",
    "1. Select the row where ```asin = B00I8KEOTM``` and print it.\n",
    "\n",
    "1. Count the total number of rows.\n",
    "\n",
    "1. Calculate the mean ```price```.\n",
    "\n",
    "1. You need to conduct the above operations, then extract some statistics out of the generated columns. You need to put the statistics in a python dictionary named ```res```. The description and schema of it are as follows:\n",
    "    ```\n",
    "    res\n",
    "     | -- count_total: int -- count of total rows of the entire table after your operations\n",
    "     | -- mean_price: float -- mean value of column price\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-26T20:45:52.368918Z",
     "start_time": "2020-01-26T20:45:52.355018Z"
    }
   },
   "outputs": [],
   "source": [
    "def task_0(data_io, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    asin_column = 'asin'\n",
    "    overall_column = 'overall'\n",
    "    # Outputs:\n",
    "    mean_rating_column = 'meanRating'\n",
    "    count_rating_column = 'countRating'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "\n",
    "    product_data.show(5)\n",
    "    product_data[['asin']].show(5)\n",
    "    product_data.where(F.col('asin') == 'B00I8KEOTM').show()\n",
    "    count_rows = product_data.count()\n",
    "    mean_price = product_data.select(F.avg(F.col('price'))).head()[0]\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    # Calculate the values programmatically. Do not change the keys and do not\n",
    "    # hard-code values in the dict. Your submission will be evaluated with\n",
    "    # different inputs.\n",
    "    # Modify the values of the following dictionary accordingly.\n",
    "    res = {'count_total': None, 'mean_price': None}\n",
    "    \n",
    "    # Modify res:\n",
    "\n",
    "    res['count_total'] = count_rows\n",
    "    res['mean_price'] = mean_price\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-26T20:45:54.222111Z",
     "start_time": "2020-01-26T20:45:52.370377Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "|      asin|           salesRank|          categories|               title|price|             related|\n",
      "+----------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "|B00I8HVV6E|{Home &amp; Kitch...|[[Home & Kitchen,...|Intelligent Desig...|27.99|{also_viewed -> [...|\n",
      "|B00I8KEOTM|                null|[[Apps for Androi...|                null| null|{also_viewed -> [...|\n",
      "|B00I8KCW4G|{Clothing -> 2233...|[[Clothing, Shoes...|eShakti Women's P...|41.95|{also_viewed -> [...|\n",
      "|B00I8JKCQW|{Clothing -> 1405...|[[Clothing, Shoes...|Lady Slimming Mid...| null|{also_viewed -> [...|\n",
      "|B00I8JKI8E|{Home &amp; Kitch...|[[Clothing, Shoes...|3 Tier Bangle Bra...|24.99|{also_viewed -> [...|\n",
      "+----------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+\n",
      "|      asin|\n",
      "+----------+\n",
      "|B00I8HVV6E|\n",
      "|B00I8KEOTM|\n",
      "|B00I8KCW4G|\n",
      "|B00I8JKCQW|\n",
      "|B00I8JKI8E|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+---------+--------------------+-----+-----+--------------------+\n",
      "|      asin|salesRank|          categories|title|price|             related|\n",
      "+----------+---------+--------------------+-----+-----+--------------------+\n",
      "|B00I8KEOTM|     null|[[Apps for Androi...| null| null|{also_viewed -> [...|\n",
      "+----------+---------+--------------------+-----+-----+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 36:==========================================>             (51 + 2) / 67]\r",
      "\r",
      "[Stage 36:=============================================>          (55 + 2) / 67]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests for task_0 --------------------------------------------------------------\n",
      "2/2 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if INPUT_FORMAT == 'dataframe':\n",
    "    res = task_0(data_io, data_dict['product'])\n",
    "    pa2.tests.test(res, 'task_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you will aggregate and extract some information from the user review table. We want to know for each\n",
    "product, what are the mean and the number of ratings it received. Implement a function task 1 that does the\n",
    "following:\n",
    "1. For each product ID asin in product, calculate the average rating it received in the column meanRating.\n",
    "The ratings are stored in column overall of review.\n",
    "3\n",
    "2. Similarly, put the count of ratings for each product in a new column named countRating.\n",
    "3. You need to conduct the above operations, then extract some statistics out of the generated columns. You\n",
    "need to put the statistics in a python dictionary named res. The description and schema of it are as follows:\n",
    "res\n",
    "| -- count_total: int -- count of rows of the transformed table, including null rows\n",
    "| -- mean_meanRating: float -- mean value of meanRating\n",
    "| -- variance_meanRating: float -- variance of ...\n",
    "| -- numNulls_meanRating: int -- count of nulls of ...\n",
    "| -- mean_countRating: float -- mean value of countRating\n",
    "| -- variance_countRating: float -- variance of ...\n",
    "| -- numNulls_countRating: int -- count of nulls of ...\n",
    "If for a product ID, there is not a single reference in review, meaning it was never reviewed, you should put\n",
    "null in both meanRating and countRating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. product\n",
    "|-- asin: string, the product id, e.g., ‘B00I8HVV6E’\n",
    "|-- salesRank: map, a map between category and sales rank, e.g., {‘Home & Kitchen’: 796318}\n",
    "| |-- key: string, category, e.g., ‘Home & Kitchen’\n",
    "| |-- value: integer, rank, e.g., 796318\n",
    "|-- categories: array, list of list of categories, e.g., [[‘Home & Kitchen’, ’Artwork’]]\n",
    "| |-- element: array, list of categories, e.g., [‘Home & Kitchen’, ’Artwork’]\n",
    "| | |-- element: string, category, e.g., ‘Home & Kitchen’\n",
    "|-- title: string, title of product, e.g., ‘Intelligent Design Cotton Canvas’\n",
    "|-- price: float, price of product, e.g., 27.9\n",
    "|-- related: map, related information, e.g., {‘also_viewed’: [‘B00I8HW0UK’]}\n",
    "| |-- key: string, the attribute name of the information, e.g., ‘also_viewed’\n",
    "| |-- value: array, array of product ids, e.g., [‘B00I8HW0UK’]\n",
    "| | |-- element: string product id , e.g., ‘B00I8HW0UK’\n",
    "2. product_processed\n",
    "|-- asin: string, same as above\n",
    "|-- title: string, title column after imputation, e.g., ‘Intelligent Design Cotton Canvas’\n",
    "|-- category: string, category column after extraction, e.g., ‘Home & Kitchen’\n",
    "3. review\n",
    "|-- asin: string, same as above\n",
    "|-- reviewerID: string, the reviewer id, e.g., ‘A1MIP8H7G33SHC’\n",
    "|-- overall: float, the rating associated with the review, e.g., 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, count, variance, col, isnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:30:44.034299Z",
     "start_time": "2019-12-10T21:30:44.012787Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_1 assignment2.py\n",
    "def task_1(data_io, review_data, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    asin_column = 'asin'\n",
    "    overall_column = 'overall'\n",
    "    # Outputs:\n",
    "    mean_rating_column = 'meanRating'\n",
    "    count_rating_column = 'countRating'\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "\n",
    "\n",
    "    grouped_reviews = review_data.groupBy(asin_column).agg(\n",
    "        F.mean(F.col(overall_column)).alias(mean_rating_column),\n",
    "        F.count(F.col(overall_column)).alias(count_rating_column)\n",
    "    )\n",
    "    \n",
    "\n",
    "    grouped_reviews.cache()\n",
    "\n",
    "\n",
    "    merged_df = product_data.join(grouped_reviews, product_data[asin_column] == grouped_reviews[asin_column], how='left').select(product_data[asin_column], mean_rating_column, count_rating_column)\n",
    "    \n",
    "\n",
    "    merged_df.cache()\n",
    "\n",
    "\n",
    "    count_total = merged_df.count()\n",
    "    mean_meanRating = merged_df.agg(F.mean(mean_rating_column)).first()[0]\n",
    "    variance_meanRating = merged_df.agg(F.variance(mean_rating_column)).first()[0]\n",
    "    numNulls_meanRating = merged_df.filter(F.col(mean_rating_column).isNull()).count()\n",
    "    mean_countRating = merged_df.agg(F.mean(count_rating_column)).first()[0]\n",
    "    variance_countRating = merged_df.agg(F.variance(count_rating_column)).first()[0]\n",
    "    numNulls_countRating = merged_df.filter(F.col(count_rating_column).isNull()).count()\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': count_total,\n",
    "        'mean_meanRating': mean_meanRating,\n",
    "        'variance_meanRating': variance_meanRating,\n",
    "        'numNulls_meanRating': numNulls_meanRating,\n",
    "        'mean_countRating': mean_countRating,\n",
    "        'variance_countRating': variance_countRating,\n",
    "        'numNulls_countRating': numNulls_countRating\n",
    "    }\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:19:04.214179Z",
     "start_time": "2019-12-09T22:18:39.293699Z"
    },
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests for task_1 --------------------------------------------------------------\n",
      "Test 1/7 : count_total ... Pass\n",
      "Test 2/7 : mean_countRating ... Pass\n",
      "Test 3/7 : mean_meanRating ... Pass\n",
      "Test 4/7 : numNulls_countRating ... Pass\n",
      "Test 5/7 : numNulls_meanRating ... Pass\n",
      "Test 6/7 : variance_countRating ... Pass\n",
      "Test 7/7 : variance_meanRating ... Pass\n",
      "7/7 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 83:====================================================> (194 + 2) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = task_1(data_io, data_dict['review'], data_dict['product'])\n",
    "pa2.tests.test(res, 'task_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 39.823331356048584 seconds\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:31:16.942833Z",
     "start_time": "2019-12-10T21:31:16.925378Z"
    }
   },
   "outputs": [],
   "source": [
    "def task_2(data_io, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    salesRank_column = 'salesRank'\n",
    "    categories_column = 'categories'\n",
    "    asin_column = 'asin'\n",
    "    # Outputs:\n",
    "    category_column = 'category'\n",
    "    bestSalesCategory_column = 'bestSalesCategory'\n",
    "    bestSalesRank_column = 'bestSalesRank'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "\n",
    "\n",
    "    extract_category = lambda x: None if x is None or len(x) == 0 or len(x[0]) == 0 or x[0][0] == '' else x[0][0]\n",
    "    udf_extract_category = F.udf(extract_category, T.StringType())\n",
    "    product_data = product_data.withColumn(\n",
    "        'category', udf_extract_category(product_data['categories']))\n",
    "    \n",
    "    \n",
    "    extract_best_category = lambda x: None if x is None or len(list(x.keys())) == 0 else list(x.keys())[0]\n",
    "    udf_extract_best_category = F.udf(extract_best_category, T.StringType())\n",
    "\n",
    "    product_data = product_data.withColumn('bestSalesCategory', udf_extract_best_category('salesRank'))\n",
    "    product_data.select('bestSalesCategory').head(5)\n",
    "    \n",
    "    \n",
    "    extract_sales_rank = lambda x: None if x is None or len(list(x.values())) == 0 else list(x.values())[0]\n",
    "    udf_extract_sales_rank = F.udf(extract_sales_rank, T.IntegerType())\n",
    "\n",
    "    product_data = product_data.withColumn('bestSalesRank', udf_extract_sales_rank('salesRank'))\n",
    "    product_data.select('bestSalesRank').head(5)\n",
    "    \n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': None,\n",
    "        'mean_bestSalesRank': None,\n",
    "        'variance_bestSalesRank': None,\n",
    "        'numNulls_category': None,\n",
    "        'countDistinct_category': None,\n",
    "        'numNulls_bestSalesCategory': None,\n",
    "        'countDistinct_bestSalesCategory': None\n",
    "    }\n",
    "    # Modify res:\n",
    "\n",
    "    res['count_total'] = product_data.count() \n",
    "    res['mean_bestSalesRank'] = product_data.agg(F.mean('bestSalesRank')).collect()[0][0] \n",
    "    res['variance_bestSalesRank'] = product_data.agg(F.variance('bestSalesRank')).collect()[0][0] \n",
    "    res['numNulls_category'] = product_data.filter(F.col('category').isNull()).count() \n",
    "    res['countDistinct_category'] = product_data.agg(F.countDistinct('category')).collect()[0][0] \n",
    "    res['numNulls_bestSalesCategory'] = product_data.filter(F.col('bestSalesCategory').isNull()).count() \n",
    "    res['countDistinct_bestSalesCategory'] = product_data.agg(F.countDistinct('bestSalesCategory')).collect()[0][0]\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_2')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:19:19.308187Z",
     "start_time": "2019-12-09T22:19:04.274345Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 88:==============(86 + -19) / 67][Stage 116:=>              (8 + 4) / 67]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/06/10 00:24:11 ERROR TaskSchedulerImpl: Lost executor 1 on 10.34.64.35: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "24/06/10 00:24:11 WARN TaskSetManager: Lost task 5.0 in stage 116.0 (TID 2326) (10.34.64.35 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "24/06/10 00:24:11 WARN TaskSetManager: Lost task 4.0 in stage 116.0 (TID 2324) (10.34.64.35 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_290_3 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_191 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_3 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_27 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_60 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_178 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_58 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_58 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_74 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_110 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_66 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_123 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_180 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_52 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_127 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_198 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_9 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_157 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_26 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_83 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_71 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_55 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_39 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_151 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_185 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_1 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_136 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_141 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_103 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_1 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_174 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_117 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_28 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_48 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_137 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_54 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_127 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_161 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_79 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_153 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_12 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_6 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_58 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_141 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_32 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_84 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_59 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_164 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_90 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_180 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_42 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_101 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_167 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_146 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_185 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_136 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_76_20 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_117 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_89 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_101 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_37 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_174 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_10 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_129 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_76_2 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_28 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_24 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_27 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_35 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_88_1 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_137 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_45 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_79 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_59 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_35 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_123 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_64 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_121 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_95 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_161 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_38 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_88_8 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_84 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_5 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_50 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_77 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_27 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_46 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_90 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_3 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_47 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_20 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_47 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_10 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_32 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_76_15 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_16 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_28 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_115 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_52 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_71 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_112 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_167 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_20 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_39 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_20 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_67 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_191 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_38 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_76_0 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_96 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_74 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_48 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_59 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_115 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_9 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_121 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_129 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_33 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_143 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_36 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_178 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_68 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_38 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_21 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_64 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_132 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_43 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_77 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_158 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_172 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_23 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_7 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_157 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_172 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_188 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_54 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_1 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_32 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_132 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_16 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_17 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_188 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_83 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_153 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_67 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_68 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_52 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_3 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_107 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_147 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_151 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_96 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_194 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_4 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_65 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_107 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_143 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_194 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_64 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_88_3 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_89 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_147 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_39 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_198 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_110 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_15 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_290_1 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_76_9 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_42 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_30 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_146 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_48 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_103 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_158 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_23 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_95 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_76_14 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_164 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_76_8 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_141_112 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_64_63 !\n",
      "24/06/10 00:24:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_54 !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.======>        (29 + 2) / 67]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/python/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/bitnami/python/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/bitnami/python/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "[Stage 88:==============(86 + -19) / 67][Stage 116:======>        (30 + 2) / 67]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mtask_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproduct\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m pa2\u001b[38;5;241m.\u001b[39mtests\u001b[38;5;241m.\u001b[39mtest(res, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask_2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 27\u001b[0m, in \u001b[0;36mtask_2\u001b[0;34m(data_io, product_data)\u001b[0m\n\u001b[1;32m     24\u001b[0m product_data\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Calculate statistics\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m count_total \u001b[38;5;241m=\u001b[39m \u001b[43mproduct_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m mean_bestSalesRank \u001b[38;5;241m=\u001b[39m product_data\u001b[38;5;241m.\u001b[39magg(F\u001b[38;5;241m.\u001b[39mmean(bestSalesRank_column))\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     29\u001b[0m variance_bestSalesRank \u001b[38;5;241m=\u001b[39m product_data\u001b[38;5;241m.\u001b[39magg(F\u001b[38;5;241m.\u001b[39mvariance(bestSalesRank_column))\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/bitnami/spark/python/pyspark/sql/dataframe.py:804\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    795\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \n\u001b[1;32m    797\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;124;03m    2\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/bitnami/python/lib/python3.8/site-packages/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/bitnami/python/lib/python3.8/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/bitnami/python/lib/python3.8/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/bitnami/python/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = task_2(data_io, data_dict['product'])\n",
    "pa2.tests.test(res, 'task_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task 3\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:31:26.542481Z",
     "start_time": "2019-12-10T21:31:26.525050Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_3 assignment2.py\n",
    "def task_3(data_io, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    asin_column = 'asin'\n",
    "    price_column = 'price'\n",
    "    attribute = 'also_viewed'\n",
    "    related_column = 'related'\n",
    "    # Outputs:\n",
    "    meanPriceAlsoViewed_column = 'meanPriceAlsoViewed'\n",
    "    countAlsoViewed_column = 'countAlsoViewed'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "\n",
    "    also_viewed_df = product_data \\\n",
    "        .select(F.col(asin_column), F.explode(F.col(f\"{related_column}.{attribute}\")).alias(attribute))\n",
    "\n",
    "\n",
    "    product_data_broadcast = F.broadcast(product_data.select(F.col(asin_column).alias(\"price_asin\"), price_column))\n",
    "\n",
    "\n",
    "    also_viewed_prices = also_viewed_df \\\n",
    "        .join(product_data_broadcast, also_viewed_df[attribute] == product_data_broadcast[\"price_asin\"], how='left')\n",
    "\n",
    "\n",
    "    also_viewed_prices.cache()\n",
    "\n",
    "\n",
    "    aggregated = also_viewed_prices \\\n",
    "        .groupBy(asin_column) \\\n",
    "        .agg(F.avg(price_column).alias(meanPriceAlsoViewed_column),\n",
    "             F.count(attribute).alias(countAlsoViewed_column))\n",
    "\n",
    "\n",
    "    result_df = product_data \\\n",
    "        .select(asin_column) \\\n",
    "        .join(aggregated, asin_column, how='left')\n",
    "\n",
    "\n",
    "    result_df.cache()\n",
    "    \n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "\n",
    "    # Modify res:\n",
    "    res = {\n",
    "        'count_total': result_df.count(),\n",
    "        'mean_meanPriceAlsoViewed': result_df.agg(F.avg(meanPriceAlsoViewed_column)).first()[0],\n",
    "        'variance_meanPriceAlsoViewed': result_df.agg(F.variance(meanPriceAlsoViewed_column)).first()[0],\n",
    "        'numNulls_meanPriceAlsoViewed': result_df.filter(F.col(meanPriceAlsoViewed_column).isNull()).count(),\n",
    "        'mean_countAlsoViewed': result_df.agg(F.avg(countAlsoViewed_column)).first()[0],\n",
    "        'variance_countAlsoViewed': result_df.agg(F.variance(countAlsoViewed_column)).first()[0],\n",
    "        'numNulls_countAlsoViewed': result_df.filter(F.col(countAlsoViewed_column).isNull()).count()\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_3')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:20:41.442745Z",
     "start_time": "2019-12-09T22:19:19.358780Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "res = task_3(data_io, data_dict['product'])\n",
    "pa2.tests.test(res, 'task_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:31:39.503390Z",
     "start_time": "2019-12-10T21:31:39.484724Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_4 assignment2.py\n",
    "def task_4(data_io, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    price_column = 'price'\n",
    "    title_column = 'title'\n",
    "    # Outputs:\n",
    "    meanImputedPrice_column = 'meanImputedPrice'\n",
    "    medianImputedPrice_column = 'medianImputedPrice'\n",
    "    unknownImputedTitle_column = 'unknownImputedTitle'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "\n",
    "    product_data = product_data.withColumn(price_column, F.col(price_column).cast(\"float\"))\n",
    "\n",
    "    # Compute mean and median of 'price' column\n",
    "    mean_price = product_data.select(F.mean(price_column)).first()[0]\n",
    "    median_price = product_data.approxQuantile(price_column, [0.5], 0.01)[0]\n",
    "\n",
    "    # Impute null values with the mean and median\n",
    "    product_data = product_data.withColumn(meanImputedPrice_column, F.when(F.col(price_column).isNull(), mean_price).otherwise(F.col(price_column)))\n",
    "    product_data = product_data.withColumn(medianImputedPrice_column, F.when(F.col(price_column).isNull(), median_price).otherwise(F.col(price_column)))\n",
    "\n",
    "    # Impute null and empty string values in 'title' column\n",
    "    product_data = product_data.withColumn(unknownImputedTitle_column, F.when(F.col(title_column).isNull(), 'unknown').otherwise(F.col(title_column)))\n",
    "    product_data = product_data.withColumn(unknownImputedTitle_column, F.when(F.col(unknownImputedTitle_column) == '', 'unknown').otherwise(F.col(unknownImputedTitle_column)))\n",
    "\n",
    "    # Aggregate results\n",
    "    count_total = product_data.count()\n",
    "    mean_meanImputedPrice = product_data.agg(F.mean(meanImputedPrice_column)).first()[0]\n",
    "    variance_meanImputedPrice = product_data.agg(F.variance(meanImputedPrice_column)).first()[0]\n",
    "    numNulls_meanImputedPrice = product_data.filter(F.col(meanImputedPrice_column).isNull()).count()\n",
    "    mean_medianImputedPrice = product_data.agg(F.mean(medianImputedPrice_column)).first()[0]\n",
    "    variance_medianImputedPrice = product_data.agg(F.variance(medianImputedPrice_column)).first()[0]\n",
    "    numNulls_medianImputedPrice = product_data.filter(F.col(medianImputedPrice_column).isNull()).count()\n",
    "    numUnknowns_unknownImputedTitle = product_data.filter(F.col(unknownImputedTitle_column) == 'unknown').count()\n",
    "\n",
    "    # Store results in a dictionary\n",
    "    res = {\n",
    "        'count_total': count_total,\n",
    "        'mean_meanImputedPrice': mean_meanImputedPrice,\n",
    "        'variance_meanImputedPrice': variance_meanImputedPrice,\n",
    "        'numNulls_meanImputedPrice': numNulls_meanImputedPrice,\n",
    "        'mean_medianImputedPrice': mean_medianImputedPrice,\n",
    "        'variance_medianImputedPrice': variance_medianImputedPrice,\n",
    "        'numNulls_medianImputedPrice': numNulls_medianImputedPrice,\n",
    "        'numUnknowns_unknownImputedTitle': numUnknowns_unknownImputedTitle\n",
    "    }\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_4')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:20:47.953226Z",
     "start_time": "2019-12-09T22:20:41.523379Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "res = task_4(data_io, data_dict['product'])\n",
    "pa2.tests.test(res, 'task_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:32:29.284661Z",
     "start_time": "2019-12-10T21:32:29.267237Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_5 assignment2.py\n",
    "def task_5(data_io, product_processed_data, word_0, word_1, word_2):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    title_column = 'title'\n",
    "    # Outputs:\n",
    "    titleArray_column = 'titleArray'\n",
    "    titleVector_column = 'titleVector'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "    product_processed_data_output = product_processed_data.withColumn(\n",
    "        titleArray_column, F.split(F.lower(F.col(title_column)), ' ')\n",
    "    )\n",
    "\n",
    "\n",
    "    word2vec = M.feature.Word2Vec(\n",
    "        minCount=100,\n",
    "        vectorSize=16,\n",
    "        seed=SEED,\n",
    "        numPartitions=4,\n",
    "        inputCol=titleArray_column,\n",
    "        outputCol='word2vec_features'\n",
    "    )\n",
    "\n",
    "    # Fit the Word2Vec model to the data\n",
    "    model = word2vec.fit(product_processed_data_output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': None,\n",
    "        'size_vocabulary': None,\n",
    "        'word_0_synonyms': [(None, None), ],\n",
    "        'word_1_synonyms': [(None, None), ],\n",
    "        'word_2_synonyms': [(None, None), ]\n",
    "    }\n",
    "    # Modify res:\n",
    "    res['count_total'] = product_processed_data_output.count()\n",
    "    res['size_vocabulary'] = model.getVectors().count()\n",
    "    for name, word in zip(\n",
    "        ['word_0_synonyms', 'word_1_synonyms', 'word_2_synonyms'],\n",
    "        [word_0, word_1, word_2]\n",
    "    ):\n",
    "        res[name] = model.findSynonymsArray(word, 10)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_5')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:26:05.015529Z",
     "start_time": "2019-12-09T22:20:47.999834Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "res = task_5(data_io, data_dict['product_processed'], 'piano', 'rice', 'laptop')\n",
    "pa2.tests.test(res, 'task_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:32:39.991460Z",
     "start_time": "2019-12-10T21:32:39.974136Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_6 assignment2.py\n",
    "def task_6(data_io, product_processed_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    category_column = 'category'\n",
    "    # Outputs:\n",
    "    categoryIndex_column = 'categoryIndex'\n",
    "    categoryOneHot_column = 'categoryOneHot'\n",
    "    categoryPCA_column = 'categoryPCA'\n",
    "    # -------------------------------------------------------------------------    \n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "\n",
    "\n",
    "    indexer = M.feature.StringIndexer(inputCol=\"category\", outputCol=\"categoryIndexed\", stringOrderType=\"frequencyDesc\")\n",
    "    encoder = M.feature.OneHotEncoder(inputCol=\"categoryIndexed\", outputCol=\"categoryOneHot\", dropLast=False)\n",
    "    pca = M.feature.PCA(k=15, inputCol=\"categoryOneHot\", outputCol=\"categoryPCA\")\n",
    "\n",
    "    pipeline = M.Pipeline(stages=[indexer, encoder, pca])\n",
    "    model = pipeline.fit(product_processed_data)\n",
    "    final = model.transform(product_processed_data)\n",
    "    \n",
    "    summarizer = M.stat.Summarizer.metrics(\"mean\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': None,\n",
    "        'meanVector_categoryOneHot': [None, ],\n",
    "        'meanVector_categoryPCA': [None, ]\n",
    "    }\n",
    "    # Modify res:\n",
    "    res['count_total'] = final.count()\n",
    "    res['meanVector_categoryOneHot'] = final.select(summarizer.summary(final.categoryOneHot)).collect()[0][0][0]\n",
    "    res['meanVector_categoryPCA'] = final.select(summarizer.summary(final.categoryPCA)).collect()[0][0][0]\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_6')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:29:57.717617Z",
     "start_time": "2019-12-09T22:29:51.132434Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "res = task_6(data_io, data_dict['product_processed'])\n",
    "pa2.tests.test(res, 'task_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T21:23:18.882119Z",
     "start_time": "2019-11-26T21:23:18.873162Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print (\"End to end time: {}\".format(time.time()-begin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring the part_2 datasets to memory and de-cache part_1 datasets.\n",
    "# Execute this once before you start working on this Part\n",
    "data_dict, _ = data_io.cache_switch(data_dict, 'part_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_7(data_io, train_data, test_data):\n",
    "    \n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "    \n",
    "    dt = M.regression.DecisionTreeRegressor(maxDepth=5)\n",
    "\n",
    "    model = dt.fit(train_data.withColumnRenamed('overall', 'label'))\n",
    "\n",
    "    predictions = model.transform(test_data)\n",
    "    \n",
    "    evaluator = M.evaluation.RegressionEvaluator(\n",
    "        labelCol=\"overall\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"rmse\"\n",
    "    )\n",
    "\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'test_rmse': None\n",
    "    }\n",
    "    # Modify res:\n",
    "    res['test_rmse'] = rmse\n",
    "    \n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_7')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = task_7(data_io, data_dict['ml_features_train'], data_dict['ml_features_test'])\n",
    "pa2.tests.test(res, 'task_7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_8(data_io, train_data, test_data):\n",
    "    \n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "    train_data, val_data = train_data.randomSplit([0.75, 0.25], seed=42)\n",
    "    \n",
    "    rmses = []\n",
    "    \n",
    "    for i, max_depth in enumerate([5,7,9,12]):\n",
    "\n",
    "        dt = M.regression.DecisionTreeRegressor(maxDepth=max_depth)\n",
    "\n",
    "        model = dt.fit(train_data.withColumnRenamed('overall', 'label'))\n",
    "\n",
    "        predictions = model.transform(val_data)\n",
    "\n",
    "        evaluator = M.evaluation.RegressionEvaluator(\n",
    "            labelCol=\"overall\",\n",
    "            predictionCol=\"prediction\",\n",
    "            metricName=\"rmse\"\n",
    "        )\n",
    "\n",
    "        rmses.append((evaluator.evaluate(predictions), i))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'test_rmse': None,\n",
    "        'valid_rmse_depth_5': None,\n",
    "        'valid_rmse_depth_7': None,\n",
    "        'valid_rmse_depth_9': None,\n",
    "        'valid_rmse_depth_12': None,\n",
    "    }\n",
    "    # Modify res:\n",
    "    res['test_rmse'] = rmses[max(rmses)[1]][0]\n",
    "    res['valid_rmse_depth_5'] = rmses[0][0]\n",
    "    res['valid_rmse_depth_7'] = rmses[1][0]\n",
    "    res['valid_rmse_depth_9'] = rmses[2][0]\n",
    "    res['valid_rmse_depth_12'] = rmses[3][0]\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_8')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = task_8(data_io, data_dict['ml_features_train'], data_dict['ml_features_test'])\n",
    "pa2.tests.test(res, 'task_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"End to end time: {}\".format(time.time()-begin))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
